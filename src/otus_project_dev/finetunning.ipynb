{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "https://github.com/qubvel/segmentation_models.pytorch/blob/master/examples/cars%20segmentation%20(camvid).ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT = Path(\"headsegmentationdataset/\")\n",
    "\n",
    "train_image_path = ROOT / \"train\"\n",
    "train_mask_path = ROOT / \"train_masks\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ALL_IMAGES = sorted(train_image_path.glob(\"*.png\"))\n",
    "ALL_MASKS = sorted(train_mask_path.glob(\"*.png\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from catalyst import utils\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class SegmentationDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        classes,\n",
    "        images: List[Path],\n",
    "        masks: List[Path] = None,\n",
    "        transforms=None\n",
    "    ) -> None:\n",
    "        self.images = images\n",
    "        self.masks = masks\n",
    "        self.transforms = transforms\n",
    "        self.n_classes = len(classes)\n",
    "        self.classes = classes\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> dict:\n",
    "        image_path = self.images[idx]\n",
    "        image = utils.imread(image_path)\n",
    "        result = {\"image\": image}\n",
    "\n",
    "        if self.masks is not None:\n",
    "            mask = utils.imread(str(self.masks[idx]))\n",
    "            mask = cv2.cvtColor(mask,cv2.COLOR_RGB2GRAY)\n",
    "            one_hot = np.zeros((mask.shape[0], mask.shape[1], self.n_classes))\n",
    "            for i, unique_value in enumerate(self.classes):\n",
    "                one_hot[:, :, i][mask == unique_value] = 1\n",
    "            result[\"mask\"] = one_hot\n",
    "\n",
    "        if self.transforms is not None:\n",
    "            result = self.transforms(**result)\n",
    "\n",
    "        result[\"filename\"] = image_path.name\n",
    "\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLASSES = [0, 29, 76, 105, 128, 150, 179, 211, 226, 255]\n",
    "dataset = SegmentationDataset(CLASSES, ALL_IMAGES, ALL_MASKS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import albumentations as albu\n",
    "from albumentations.pytorch import ToTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_transforms(image_size=224):\n",
    "    return [albu.Resize(image_size, image_size, p=1)]\n",
    "\n",
    "def post_transforms():\n",
    "    return [albu.Normalize(), ToTensor(num_classes=9)]\n",
    "\n",
    "\n",
    "def hard_transforms():\n",
    "    result = [\n",
    "      albu.Cutout(),\n",
    "      albu.RandomBrightnessContrast(\n",
    "          brightness_limit=0.1, contrast_limit=0.1, p=0.3\n",
    "      ),\n",
    "      albu.GridDistortion(p=0.3),\n",
    "    ]\n",
    "\n",
    "    return result\n",
    "\n",
    "def compose(transforms_to_compose):\n",
    "    # combine all augmentations into single pipeline\n",
    "    result = albu.Compose([\n",
    "      item for sublist in transforms_to_compose for item in sublist\n",
    "    ])\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transforms = compose([\n",
    "    pre_transforms(),\n",
    "    hard_transforms(),\n",
    "    post_transforms(),\n",
    "])\n",
    "valid_transforms = compose([pre_transforms(), post_transforms()])\n",
    "visualize_transforms = compose([pre_transforms(), hard_transforms()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def get_loaders(\n",
    "    images: List[Path],\n",
    "    masks: List[Path],\n",
    "    random_state: int,\n",
    "    valid_size: float = 0.2,\n",
    "    batch_size: int = 32,\n",
    "    num_workers: int = 4,\n",
    "    train_transforms_fn = None,\n",
    "    valid_transforms_fn = None,\n",
    ") -> dict:\n",
    "\n",
    "    indices = np.arange(len(images))\n",
    "\n",
    "    # Let's divide the data set into train and valid parts.\n",
    "    train_indices, valid_indices = train_test_split(\n",
    "      indices, test_size=valid_size, random_state=random_state, shuffle=True\n",
    "    )\n",
    "\n",
    "    np_images = np.array(images)\n",
    "    np_masks = np.array(masks)\n",
    "\n",
    "    # Creates our train dataset\n",
    "    train_dataset = SegmentationDataset(\n",
    "      classes = CLASSES,\n",
    "      images = np_images[train_indices].tolist(),\n",
    "      masks = np_masks[train_indices].tolist(),\n",
    "      transforms = train_transforms_fn\n",
    "    )\n",
    "\n",
    "    # Creates our valid dataset\n",
    "    valid_dataset = SegmentationDataset(\n",
    "      classes = CLASSES,\n",
    "      images = np_images[valid_indices].tolist(),\n",
    "      masks = np_masks[valid_indices].tolist(),\n",
    "      transforms = valid_transforms_fn\n",
    "    )\n",
    "\n",
    "    # Catalyst uses torch.data.DataLoader\n",
    "    train_loader = DataLoader(\n",
    "      train_dataset,\n",
    "      batch_size=batch_size,\n",
    "      shuffle=True,\n",
    "      num_workers=num_workers,\n",
    "      drop_last=True,\n",
    "    )\n",
    "\n",
    "    valid_loader = DataLoader(\n",
    "      valid_dataset,\n",
    "      batch_size=batch_size,\n",
    "      shuffle=False,\n",
    "      num_workers=num_workers,\n",
    "      drop_last=True,\n",
    "    )\n",
    "\n",
    "    # And expect to get an OrderedDict of loaders\n",
    "    loaders = collections.OrderedDict()\n",
    "    loaders[\"train\"] = train_loader\n",
    "    loaders[\"valid\"] = valid_loader\n",
    "\n",
    "    return loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "random_state = 16\n",
    "\n",
    "loaders = get_loaders(\n",
    "    images=ALL_IMAGES,\n",
    "    masks=ALL_MASKS,\n",
    "    random_state=random_state,\n",
    "    train_transforms_fn=train_transforms,\n",
    "    valid_transforms_fn=valid_transforms,\n",
    "    batch_size=batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def set_requires_grad(model, value=False):\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import segmentation_models_pytorch as smp\n",
    "CLASSES = [0, 29, 76, 105, 128, 150, 179, 211, 226, 255]\n",
    "aux_params= None\n",
    "model = smp.FPN(encoder_name=\"efficientnet-b4\", classes=len(CLASSES), aux_params=aux_params)\n",
    "model_state = torch.load(\"./logs/segmentation/b4_tr_1/checkpoints/best_full.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(model_state['model_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FPNDecoder(\n",
       "  (p5): Conv2d(448, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "  (p4): FPNBlock(\n",
       "    (skip_conv): Conv2d(160, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "  )\n",
       "  (p3): FPNBlock(\n",
       "    (skip_conv): Conv2d(56, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "  )\n",
       "  (p2): FPNBlock(\n",
       "    (skip_conv): Conv2d(32, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "  )\n",
       "  (seg_blocks): ModuleList(\n",
       "    (0): SegmentationBlock(\n",
       "      (block): Sequential(\n",
       "        (0): Conv3x3GNReLU(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (1): GroupNorm(32, 128, eps=1e-05, affine=True)\n",
       "            (2): ReLU(inplace=True)\n",
       "          )\n",
       "        )\n",
       "        (1): Conv3x3GNReLU(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (1): GroupNorm(32, 128, eps=1e-05, affine=True)\n",
       "            (2): ReLU(inplace=True)\n",
       "          )\n",
       "        )\n",
       "        (2): Conv3x3GNReLU(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (1): GroupNorm(32, 128, eps=1e-05, affine=True)\n",
       "            (2): ReLU(inplace=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (1): SegmentationBlock(\n",
       "      (block): Sequential(\n",
       "        (0): Conv3x3GNReLU(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (1): GroupNorm(32, 128, eps=1e-05, affine=True)\n",
       "            (2): ReLU(inplace=True)\n",
       "          )\n",
       "        )\n",
       "        (1): Conv3x3GNReLU(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (1): GroupNorm(32, 128, eps=1e-05, affine=True)\n",
       "            (2): ReLU(inplace=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (2): SegmentationBlock(\n",
       "      (block): Sequential(\n",
       "        (0): Conv3x3GNReLU(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (1): GroupNorm(32, 128, eps=1e-05, affine=True)\n",
       "            (2): ReLU(inplace=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (3): SegmentationBlock(\n",
       "      (block): Sequential(\n",
       "        (0): Conv3x3GNReLU(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (1): GroupNorm(32, 128, eps=1e-05, affine=True)\n",
       "            (2): ReLU(inplace=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (merge): MergeBlock()\n",
       "  (dropout): Dropout2d(p=0.2, inplace=True)\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "model.encoder.requires_grad_(True)\n",
    "model.decoder.requires_grad_(True)\n",
    "# model.segmentation_head = head\n",
    "\n",
    "# print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# model.segmentation_head = head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "from catalyst.contrib.nn import DiceLoss, IoULoss, FocalLossBinary\n",
    "\n",
    "# we have multiple criterions\n",
    "criterion = {\n",
    "    \"dice\": DiceLoss(),\n",
    "    \"iou\": IoULoss(threshold=0.5),\n",
    "    \"bce\": nn.BCEWithLogitsLoss(),\n",
    "    \"focal\": FocalLossBinary(threshold=0.5)\n",
    "    # TODO change to MultyClass\n",
    "}\n",
    "#https://discuss.pytorch.org/t/is-this-a-correct-implementation-for-focal-loss-in-pytorch/43327/14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "\n",
    "from catalyst.contrib.nn import RAdam, Lookahead\n",
    "\n",
    "encoder_learning_rate = 0.001\n",
    "\n",
    "# Since we use a pre-trained encoder, we will reduce the learning rate on it.\n",
    "# weight_decay - parameter adds a L2 penalty to the cost which can effectively lead to to smaller model weights\n",
    "layerwise_params = {\"encoder*\": dict(lr=encoder_learning_rate, weight_decay=0.0001)}\n",
    "\n",
    "# This function removes weight_decay for biases and applies our layerwise_params\n",
    "model_params = utils.process_model_params(model, layerwise_params=layerwise_params)\n",
    "\n",
    "# Catalyst has new SOTA optimizers out of box\n",
    "learning_rate = 0.01\n",
    "base_optimizer = RAdam(model_params, lr=learning_rate, weight_decay=0.0001)\n",
    "optimizer = Lookahead(base_optimizer)\n",
    "\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.5, patience=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda\n"
     ]
    }
   ],
   "source": [
    "from catalyst.dl import SupervisedRunner\n",
    "\n",
    "num_epochs = 15\n",
    "logdir = \"./logs/segmentation/b4_tr_f1/\"\n",
    "\n",
    "device = utils.get_device()\n",
    "print(f\"device: {device}\")\n",
    "\n",
    "runner = SupervisedRunner(device=device, input_key=\"image\", input_target_key=\"mask\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from catalyst.dl.callbacks import DiceCallback, IouCallback, \\\n",
    "  CriterionCallback, MetricAggregationCallback\n",
    "\n",
    "callbacks = [\n",
    "    # Each criterion is calculated separately.\n",
    "    CriterionCallback(\n",
    "        input_key=\"mask\",\n",
    "        prefix=\"loss_dice\",\n",
    "        criterion_key=\"dice\"\n",
    "    ),\n",
    "    CriterionCallback(\n",
    "        input_key=\"mask\",\n",
    "        prefix=\"loss_iou\",\n",
    "        criterion_key=\"iou\"\n",
    "    ),\n",
    "    CriterionCallback(\n",
    "        input_key=\"mask\",\n",
    "        prefix=\"loss_bce\",\n",
    "        criterion_key=\"bce\"\n",
    "    ),\n",
    "    CriterionCallback(\n",
    "        input_key=\"mask\",\n",
    "        prefix=\"loss_focal\",\n",
    "        criterion_key=\"focal\"\n",
    "    ),\n",
    "\n",
    "    # And only then we aggregate everything into one loss.\n",
    "    MetricAggregationCallback(\n",
    "        prefix=\"loss\",\n",
    "        mode=\"weighted_sum\", # can be \"sum\", \"weighted_sum\" or \"mean\"\n",
    "        # because we want weighted sum, we need to add scale for each loss\n",
    "        metrics={\"loss_dice\": 1.0, \"loss_iou\": 1.0, \"loss_bce\": 0.8, \"loss_focal\":1.0},\n",
    "    ),\n",
    "\n",
    "    # metrics\n",
    "    DiceCallback(input_key=\"mask\"),\n",
    "    IouCallback(input_key=\"mask\"),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/15 * Epoch (train): 100% 330/330 [03:49<00:00,  1.44it/s, dice=0.966, iou=0.934, loss=0.107, loss_bce=0.016, loss_dice=0.034, loss_focal=0.005, loss_iou=0.055]\n",
      "1/15 * Epoch (valid): 100% 82/82 [00:17<00:00,  4.59it/s, dice=0.937, iou=0.882, loss=0.219, loss_bce=0.040, loss_dice=0.063, loss_focal=0.015, loss_iou=0.108]\n",
      "[2020-09-20 23:20:08,422] \n",
      "1/15 * Epoch 16 (_base): lr=0.0001 | momentum=0.9000\n",
      "1/15 * Epoch 16 (train): dice=0.9587 | iou=0.9207 | loss=0.1275 | loss_bce=0.0186 | loss_dice=0.0413 | loss_focal=0.0056 | loss_iou=0.0657\n",
      "1/15 * Epoch 16 (valid): dice=0.9283 | iou=0.8667 | loss=0.2639 | loss_bce=0.0577 | loss_dice=0.0717 | loss_focal=0.0236 | loss_iou=0.1225\n",
      "2/15 * Epoch (train): 100% 330/330 [03:52<00:00,  1.42it/s, dice=0.954, iou=0.912, loss=0.143, loss_bce=0.022, loss_dice=0.046, loss_focal=0.006, loss_iou=0.073]\n",
      "2/15 * Epoch (valid): 100% 82/82 [00:17<00:00,  4.59it/s, dice=0.937, iou=0.882, loss=0.219, loss_bce=0.040, loss_dice=0.063, loss_focal=0.015, loss_iou=0.109]\n",
      "[2020-09-20 23:24:22,557] \n",
      "2/15 * Epoch 17 (_base): lr=6.250e-05 | momentum=0.9000\n",
      "2/15 * Epoch 17 (train): dice=0.9588 | iou=0.9209 | loss=0.1274 | loss_bce=0.0186 | loss_dice=0.0412 | loss_focal=0.0056 | loss_iou=0.0657\n",
      "2/15 * Epoch 17 (valid): dice=0.9281 | iou=0.8664 | loss=0.2634 | loss_bce=0.0570 | loss_dice=0.0719 | loss_focal=0.0233 | loss_iou=0.1226\n",
      "3/15 * Epoch (train): 100% 330/330 [03:53<00:00,  1.41it/s, dice=0.963, iou=0.929, loss=0.112, loss_bce=0.015, loss_dice=0.037, loss_focal=0.004, loss_iou=0.059]\n",
      "3/15 * Epoch (valid): 100% 82/82 [00:18<00:00,  4.54it/s, dice=0.939, iou=0.884, loss=0.209, loss_bce=0.035, loss_dice=0.061, loss_focal=0.013, loss_iou=0.107]\n",
      "[2020-09-20 23:28:35,404] \n",
      "3/15 * Epoch 18 (_base): lr=6.250e-05 | momentum=0.9000\n",
      "3/15 * Epoch 18 (train): dice=0.9594 | iou=0.9221 | loss=0.1252 | loss_bce=0.0182 | loss_dice=0.0406 | loss_focal=0.0054 | loss_iou=0.0646\n",
      "3/15 * Epoch 18 (valid): dice=0.9285 | iou=0.8669 | loss=0.2641 | loss_bce=0.0578 | loss_dice=0.0715 | loss_focal=0.0237 | loss_iou=0.1226\n",
      "4/15 * Epoch (train): 100% 330/330 [03:53<00:00,  1.41it/s, dice=0.962, iou=0.927, loss=0.116, loss_bce=0.017, loss_dice=0.038, loss_focal=0.005, loss_iou=0.060]\n",
      "4/15 * Epoch (valid): 100% 82/82 [00:18<00:00,  4.52it/s, dice=0.938, iou=0.884, loss=0.213, loss_bce=0.037, loss_dice=0.062, loss_focal=0.014, loss_iou=0.107]\n",
      "[2020-09-20 23:32:49,053] \n",
      "4/15 * Epoch 19 (_base): lr=6.250e-05 | momentum=0.9000\n",
      "4/15 * Epoch 19 (train): dice=0.9601 | iou=0.9233 | loss=0.1232 | loss_bce=0.0179 | loss_dice=0.0399 | loss_focal=0.0053 | loss_iou=0.0636\n",
      "4/15 * Epoch 19 (valid): dice=0.9286 | iou=0.8672 | loss=0.2665 | loss_bce=0.0597 | loss_dice=0.0714 | loss_focal=0.0247 | loss_iou=0.1226\n",
      "5/15 * Epoch (train): 100% 330/330 [03:54<00:00,  1.41it/s, dice=0.971, iou=0.943, loss=0.090, loss_bce=0.012, loss_dice=0.029, loss_focal=0.004, loss_iou=0.047]\n",
      "5/15 * Epoch (valid): 100% 82/82 [00:18<00:00,  4.49it/s, dice=0.938, iou=0.883, loss=0.216, loss_bce=0.039, loss_dice=0.062, loss_focal=0.015, loss_iou=0.108]\n",
      "[2020-09-20 23:37:03,091] \n",
      "5/15 * Epoch 20 (_base): lr=6.250e-05 | momentum=0.9000\n",
      "5/15 * Epoch 20 (train): dice=0.9604 | iou=0.9239 | loss=0.1222 | loss_bce=0.0178 | loss_dice=0.0396 | loss_focal=0.0053 | loss_iou=0.0631\n",
      "5/15 * Epoch 20 (valid): dice=0.9287 | iou=0.8674 | loss=0.2653 | loss_bce=0.0593 | loss_dice=0.0713 | loss_focal=0.0245 | loss_iou=0.1222\n",
      "6/15 * Epoch (train): 100% 330/330 [03:54<00:00,  1.41it/s, dice=0.970, iou=0.941, loss=0.091, loss_bce=0.013, loss_dice=0.030, loss_focal=0.004, loss_iou=0.047]\n",
      "6/15 * Epoch (valid): 100% 82/82 [00:18<00:00,  4.49it/s, dice=0.938, iou=0.883, loss=0.216, loss_bce=0.040, loss_dice=0.062, loss_focal=0.015, loss_iou=0.107]\n",
      "[2020-09-20 23:41:17,702] \n",
      "6/15 * Epoch 21 (_base): lr=3.125e-05 | momentum=0.9000\n",
      "6/15 * Epoch 21 (train): dice=0.9606 | iou=0.9243 | loss=0.1215 | loss_bce=0.0176 | loss_dice=0.0394 | loss_focal=0.0053 | loss_iou=0.0627\n",
      "6/15 * Epoch 21 (valid): dice=0.9290 | iou=0.8679 | loss=0.2665 | loss_bce=0.0605 | loss_dice=0.0710 | loss_focal=0.0252 | loss_iou=0.1220\n",
      "7/15 * Epoch (train): 100% 330/330 [03:55<00:00,  1.40it/s, dice=0.959, iou=0.922, loss=0.124, loss_bce=0.018, loss_dice=0.041, loss_focal=0.005, loss_iou=0.064]\n",
      "7/15 * Epoch (valid): 100% 82/82 [00:18<00:00,  4.53it/s, dice=0.939, iou=0.885, loss=0.211, loss_bce=0.038, loss_dice=0.061, loss_focal=0.014, loss_iou=0.106]\n",
      "[2020-09-20 23:45:32,851] \n",
      "7/15 * Epoch 22 (_base): lr=3.125e-05 | momentum=0.9000\n",
      "7/15 * Epoch 22 (train): dice=0.9609 | iou=0.9247 | loss=0.1208 | loss_bce=0.0175 | loss_dice=0.0391 | loss_focal=0.0052 | loss_iou=0.0624\n",
      "7/15 * Epoch 22 (valid): dice=0.9295 | iou=0.8687 | loss=0.2653 | loss_bce=0.0604 | loss_dice=0.0705 | loss_focal=0.0251 | loss_iou=0.1213\n",
      "8/15 * Epoch (train): 100% 330/330 [03:54<00:00,  1.40it/s, dice=0.962, iou=0.926, loss=0.118, loss_bce=0.016, loss_dice=0.038, loss_focal=0.005, loss_iou=0.062]\n",
      "8/15 * Epoch (valid): 100% 82/82 [00:18<00:00,  4.49it/s, dice=0.939, iou=0.885, loss=0.209, loss_bce=0.037, loss_dice=0.061, loss_focal=0.014, loss_iou=0.106]\n",
      "[2020-09-20 23:49:47,919] \n",
      "8/15 * Epoch 23 (_base): lr=3.125e-05 | momentum=0.9000\n",
      "8/15 * Epoch 23 (train): dice=0.9613 | iou=0.9255 | loss=0.1194 | loss_bce=0.0173 | loss_dice=0.0387 | loss_focal=0.0051 | loss_iou=0.0617\n",
      "8/15 * Epoch 23 (valid): dice=0.9292 | iou=0.8683 | loss=0.2660 | loss_bce=0.0606 | loss_dice=0.0708 | loss_focal=0.0252 | loss_iou=0.1215\n",
      "9/15 * Epoch (train): 100% 330/330 [03:55<00:00,  1.40it/s, dice=0.957, iou=0.918, loss=0.133, loss_bce=0.020, loss_dice=0.043, loss_focal=0.006, loss_iou=0.068]\n",
      "9/15 * Epoch (valid): 100% 82/82 [00:18<00:00,  4.46it/s, dice=0.939, iou=0.884, loss=0.213, loss_bce=0.038, loss_dice=0.061, loss_focal=0.015, loss_iou=0.106]\n",
      "[2020-09-20 23:54:02,870] \n",
      "9/15 * Epoch 24 (_base): lr=3.125e-05 | momentum=0.9000\n",
      "9/15 * Epoch 24 (train): dice=0.9611 | iou=0.9251 | loss=0.1201 | loss_bce=0.0174 | loss_dice=0.0389 | loss_focal=0.0052 | loss_iou=0.0621\n",
      "9/15 * Epoch 24 (valid): dice=0.9290 | iou=0.8679 | loss=0.2660 | loss_bce=0.0602 | loss_dice=0.0710 | loss_focal=0.0250 | loss_iou=0.1217\n",
      "10/15 * Epoch (train): 100% 330/330 [03:55<00:00,  1.40it/s, dice=0.964, iou=0.931, loss=0.110, loss_bce=0.016, loss_dice=0.036, loss_focal=0.005, loss_iou=0.057]\n",
      "10/15 * Epoch (valid): 100% 82/82 [00:18<00:00,  4.48it/s, dice=0.938, iou=0.884, loss=0.216, loss_bce=0.040, loss_dice=0.062, loss_focal=0.015, loss_iou=0.107]\n",
      "[2020-09-20 23:58:17,487] \n",
      "10/15 * Epoch 25 (_base): lr=1.563e-05 | momentum=0.9000\n",
      "10/15 * Epoch 25 (train): dice=0.9613 | iou=0.9255 | loss=0.1194 | loss_bce=0.0173 | loss_dice=0.0387 | loss_focal=0.0051 | loss_iou=0.0617\n",
      "10/15 * Epoch 25 (valid): dice=0.9289 | iou=0.8677 | loss=0.2681 | loss_bce=0.0614 | loss_dice=0.0711 | loss_focal=0.0256 | loss_iou=0.1222\n",
      "11/15 * Epoch (train): 100% 330/330 [03:55<00:00,  1.40it/s, dice=0.963, iou=0.929, loss=0.112, loss_bce=0.016, loss_dice=0.037, loss_focal=0.005, loss_iou=0.059]\n",
      "11/15 * Epoch (valid): 100% 82/82 [00:18<00:00,  4.50it/s, dice=0.939, iou=0.885, loss=0.211, loss_bce=0.038, loss_dice=0.061, loss_focal=0.015, loss_iou=0.105]\n",
      "[2020-09-21 00:02:32,786] \n",
      "11/15 * Epoch 26 (_base): lr=1.563e-05 | momentum=0.9000\n",
      "11/15 * Epoch 26 (train): dice=0.9618 | iou=0.9264 | loss=0.1178 | loss_bce=0.0170 | loss_dice=0.0382 | loss_focal=0.0050 | loss_iou=0.0609\n",
      "11/15 * Epoch 26 (valid): dice=0.9294 | iou=0.8686 | loss=0.2682 | loss_bce=0.0625 | loss_dice=0.0706 | loss_focal=0.0262 | loss_iou=0.1214\n",
      "12/15 * Epoch (train): 100% 330/330 [03:56<00:00,  1.39it/s, dice=0.960, iou=0.923, loss=0.122, loss_bce=0.017, loss_dice=0.040, loss_focal=0.005, loss_iou=0.063]\n",
      "12/15 * Epoch (valid): 100% 82/82 [00:18<00:00,  4.50it/s, dice=0.940, iou=0.886, loss=0.209, loss_bce=0.037, loss_dice=0.060, loss_focal=0.014, loss_iou=0.105]\n",
      "[2020-09-21 00:06:49,360] \n",
      "12/15 * Epoch 27 (_base): lr=1.563e-05 | momentum=0.9000\n",
      "12/15 * Epoch 27 (train): dice=0.9617 | iou=0.9262 | loss=0.1184 | loss_bce=0.0172 | loss_dice=0.0383 | loss_focal=0.0051 | loss_iou=0.0612\n",
      "12/15 * Epoch 27 (valid): dice=0.9291 | iou=0.8680 | loss=0.2672 | loss_bce=0.0611 | loss_dice=0.0709 | loss_focal=0.0255 | loss_iou=0.1219\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13/15 * Epoch (train): 100% 330/330 [03:55<00:00,  1.40it/s, dice=0.962, iou=0.927, loss=0.110, loss_bce=0.016, loss_dice=0.038, loss_focal=0.004, loss_iou=0.055]\n",
      "13/15 * Epoch (valid): 100% 82/82 [00:18<00:00,  4.54it/s, dice=0.940, iou=0.888, loss=0.206, loss_bce=0.037, loss_dice=0.060, loss_focal=0.014, loss_iou=0.104]\n",
      "[2020-09-21 00:11:04,535] \n",
      "13/15 * Epoch 28 (_base): lr=1.563e-05 | momentum=0.9000\n",
      "13/15 * Epoch 28 (train): dice=0.9618 | iou=0.9265 | loss=0.1176 | loss_bce=0.0170 | loss_dice=0.0382 | loss_focal=0.0050 | loss_iou=0.0607\n",
      "13/15 * Epoch 28 (valid): dice=0.9298 | iou=0.8692 | loss=0.2675 | loss_bce=0.0627 | loss_dice=0.0702 | loss_focal=0.0263 | loss_iou=0.1208\n",
      "14/15 * Epoch (train): 100% 330/330 [03:54<00:00,  1.41it/s, dice=0.960, iou=0.923, loss=0.127, loss_bce=0.019, loss_dice=0.040, loss_focal=0.006, loss_iou=0.066]\n",
      "14/15 * Epoch (valid): 100% 82/82 [00:17<00:00,  4.62it/s, dice=0.940, iou=0.886, loss=0.209, loss_bce=0.037, loss_dice=0.060, loss_focal=0.014, loss_iou=0.105]\n",
      "[2020-09-21 00:15:18,374] \n",
      "14/15 * Epoch 29 (_base): lr=7.813e-06 | momentum=0.9000\n",
      "14/15 * Epoch 29 (train): dice=0.9618 | iou=0.9265 | loss=0.1179 | loss_bce=0.0171 | loss_dice=0.0382 | loss_focal=0.0051 | loss_iou=0.0610\n",
      "14/15 * Epoch 29 (valid): dice=0.9292 | iou=0.8682 | loss=0.2668 | loss_bce=0.0611 | loss_dice=0.0708 | loss_focal=0.0254 | loss_iou=0.1217\n",
      "15/15 * Epoch (train): 100% 330/330 [03:51<00:00,  1.43it/s, dice=0.965, iou=0.933, loss=0.105, loss_bce=0.015, loss_dice=0.035, loss_focal=0.004, loss_iou=0.054]\n",
      "15/15 * Epoch (valid): 100% 82/82 [00:17<00:00,  4.56it/s, dice=0.940, iou=0.886, loss=0.209, loss_bce=0.038, loss_dice=0.060, loss_focal=0.014, loss_iou=0.104]\n",
      "[2020-09-21 00:19:28,869] \n",
      "15/15 * Epoch 30 (_base): lr=7.813e-06 | momentum=0.9000\n",
      "15/15 * Epoch 30 (train): dice=0.9620 | iou=0.9268 | loss=0.1170 | loss_bce=0.0168 | loss_dice=0.0380 | loss_focal=0.0050 | loss_iou=0.0606\n",
      "15/15 * Epoch 30 (valid): dice=0.9295 | iou=0.8687 | loss=0.2674 | loss_bce=0.0620 | loss_dice=0.0705 | loss_focal=0.0260 | loss_iou=0.1213\n",
      "Top best models:\n",
      "logs/segmentation/b4_tr_f1/checkpoints/train.13.pth\t0.8692\n"
     ]
    }
   ],
   "source": [
    "runner.train(\n",
    "    model=model,\n",
    "    criterion=criterion,\n",
    "    optimizer=optimizer,\n",
    "    scheduler=scheduler,\n",
    "    # our dataloaders\n",
    "    loaders=loaders,\n",
    "    # We can specify the callbacks list for the experiment;\n",
    "    callbacks=callbacks,\n",
    "    # path to save logs\n",
    "    logdir=logdir,\n",
    "    num_epochs=num_epochs,\n",
    "    # save our best checkpoint by IoU metric\n",
    "    main_metric=\"iou\",\n",
    "    # IoU needs to be maximized.\n",
    "    minimize_metric=False,\n",
    "    # for FP16. It uses the variable from the very first cell\n",
    "    fp16=None,\n",
    "    # prints train logs\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# from pathlib import Path\n",
    "# ROOT = Path(\"headsegmentationdataset/\")\n",
    "# test_image_path = ROOT / \"test3\"\n",
    "# TEST_IMAGES = sorted(test_image_path.glob(\"*.jpg\"))\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# # create test dataset\n",
    "# test_dataset = SegmentationDataset(\n",
    "#     CLASSES,\n",
    "#     TEST_IMAGES,\n",
    "#     transforms=valid_transforms\n",
    "# )\n",
    "#\n",
    "# num_workers: int = 4\n",
    "#\n",
    "# infer_loader = DataLoader(\n",
    "#     test_dataset,\n",
    "#     batch_size=batch_size,\n",
    "#     shuffle=False,\n",
    "#     num_workers=num_workers\n",
    "# )\n",
    "#\n",
    "# # this get predictions for the whole loader\n",
    "# predictions = np.vstack(list(map(\n",
    "#     lambda x: x[\"logits\"].cpu().numpy(),\n",
    "#     runner.predict_loader(loader=infer_loader, resume=f\"{logdir}/checkpoints/best.pth\")\n",
    "#     # runner.predict_loader(loader=infer_loader, resume=f\"{logdir}/checkpoints/train.1.exception_KeyboardInterrupt.pth\")\n",
    "# )))\n",
    "#\n",
    "# print(type(predictions))\n",
    "# print(predictions.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import numpy\n",
    "# # p = predictions[0].permute(1, 2, 0)\n",
    "# # print (p.shape)\n",
    "# import torch\n",
    "# max_count = 62\n",
    "# #\n",
    "# # masks = []\n",
    "# for i, (features, logits) in enumerate(zip(test_dataset, predictions)):\n",
    "#     # print(features[\"image\"])\n",
    "# #     image = utils.tensor_to_ndimage(features[\"image\"])\n",
    "# #\n",
    "#     # slices\n",
    "#     print(torch.from_numpy(logits))\n",
    "#     res = torch.argmax(torch.from_numpy(logits), dim=0)\n",
    "#     print(res.shape)\n",
    "#     res = numpy.asarray(res)\n",
    "#     v = 255/len(CLASSES)\n",
    "#     res = res*v - 1\n",
    "#     res = res.astype(\"int\")\n",
    "#\n",
    "#     plt.figure(figsize=(5, 7))\n",
    "#     plt.subplot(1, 2, 1)\n",
    "#     plt.imshow(res, cmap='gray', vmin=0, vmax=255)\n",
    "#     plt.title(f\"res\")\n",
    "#\n",
    "#     # image is ok\n",
    "#     image = utils.tensor_to_ndimage(features[\"image\"])\n",
    "#     plt.figure(figsize=(5, 7))\n",
    "#     plt.subplot(1, 2, 1)\n",
    "#     plt.imshow(image)\n",
    "#     plt.title(f\"Image\")\n",
    "#\n",
    "#\n",
    "#\n",
    "#     if i >= max_count:\n",
    "#         break\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
